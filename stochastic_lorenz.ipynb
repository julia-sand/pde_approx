{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ready"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "function nn_estimation(file_name)\n",
    "        \n",
    "    #parameters of the function\n",
    "    d = Int(3); #number of dimensions\n",
    "    N = 100; #number of timesteps of SDE evolution\n",
    "    T = Int(1);  #ttm\n",
    "    h = T / N #size of a timestep\n",
    "    \n",
    "    #drift and volatility terms\n",
    "    alpha_1, alpha_2, alpha_3 = 10., 14., 8/3\n",
    "    beta = ones(3)*0.15\n",
    "    \n",
    "    batch_samples = Int(1024); #total samples in batch\n",
    "    mc_test_batch = Int(500); #number of samples in testing batches\n",
    "    \n",
    "    function X_sampler(batch_samples)\n",
    "        x_1 = rand(Uniform(0.5,2.5),(1,batch_samples))\n",
    "        x_2 = rand(Uniform(8,10),(1,batch_samples))\n",
    "        x_3 = rand(Uniform(10,12),(1,batch_samples))\n",
    "        x = Array(vcat(x_1,x_2,x_3))\n",
    "    end\n",
    "    \n",
    "    function mu(x)\n",
    "        x_1 = Array(transpose(x[1,:]))\n",
    "        x_2 = Array(transpose(x[2,:]))\n",
    "        x_3 = Array(transpose(x[3,:]))\n",
    "        mu_1 = alpha_1 .* (x_2 .- x_1)\n",
    "        mu_2 = alpha_2 .* x_1 .- x_2 .- x_3 \n",
    "        mu_3 = x_1 .*x_2 .- alpha_3 .*x_3\n",
    "        mu_x = Array(vcat(mu_1,mu_2,mu_3))\n",
    "        return mu_x\n",
    "    end\n",
    "\n",
    "    mc_samples = Int(20000); #number of samples to take for Monte-Carlo approximation\n",
    "    mc_exp_rounds = Int(5); #number of times to repeat MC for the error average\n",
    "    \n",
    "    learning_rate = Float16(0.001); #initial learning rate\n",
    "    learn_rate_decrease = 250000; #how frequently to decay learning rate\n",
    "\n",
    "    train_steps = Int(750000); #total number of training epochs\n",
    "    err_step = Int(25000); #after how many training steps to compare errors\n",
    "    \n",
    " #   #standard normalisation function\n",
    " #   function norm_ab(y)\n",
    " #       mid_point = (a + b) / 2\n",
    " #       y_norm(y) = (y .- mid_point) ./ (b-a)\n",
    " #       return mapslices(y_norm,y;dims =1)\n",
    " #   end\n",
    "        \n",
    "    function x_sde(x,batch_samples)\n",
    "        for n in 1:N\n",
    "            norm_mu = sqrt.(sum(mu(x).^2;dims =1))\n",
    "            ind = ifelse.(norm_mu .> (N/T),0,1)\n",
    "            #sampled brownian motion\n",
    "            eps = rand(Normal(0,1),(3,batch_samples))\n",
    "            x = x .+ (mu(x).*h).*ind .+ beta .* eps\n",
    "        end\n",
    "        return x\n",
    "    end\n",
    "\n",
    "\n",
    "    function x_phi(x::Array) #function to use with FK expectation\n",
    "        sum(x .^2; dims =1)\n",
    "    end;\n",
    "\n",
    "    #initialise error file and create row headers\n",
    "    df_row = DataFrame(step = \"step\",\n",
    "                        l1_errs=\"l1_errs\",l2_errs=\"l2_errs\",li_errs=\"li_errs\",\n",
    "                        rel_l1_errs=\"rel_l1_errs\",rel_l2_errs=\"rel_l2_errs\",rel_li_errs=\"rel_li_errs\",\n",
    "                        t_nn=\"t_nn\",t_mc =\"t_mc\")\n",
    "    \n",
    "    CSV.write(file_name, df_row, append = true);\n",
    "    \n",
    "    #calculate errors and write to file\n",
    "    function k_iter_output(t_nn, k)\n",
    "        \n",
    "        #generate mc data\n",
    "        function mc_sampler(x_val, mc_test_batch)\n",
    "            x_mc_store = zeros((1,mc_test_batch))\n",
    "            for _ in 1:mc_samples\n",
    "                x_mc_store += Array((x_phi(x_sde(x_val,mc_test_batch))))\n",
    "            end\n",
    "            phi_mc = x_mc_store ./ mc_samples;\n",
    "            return Array(phi_mc)\n",
    "        end\n",
    "           \n",
    "        #find the expected error vs mc samples\n",
    "        #initial errors for finding the mean\n",
    "        t_mc = 0 \n",
    "        l1_errs,l2_errs,li_errs = 0.,0.,0.\n",
    "        rel_l1_errs, rel_l2_errs, rel_li_errs = 0., 0., 0.\n",
    "            \n",
    "        for _ in 1:mc_exp_rounds\n",
    "            \n",
    "            x_val = X_sampler(mc_test_batch)\n",
    "\n",
    "            #run through testmode NN for comparison\n",
    "            X_0 = Array(normalise(x_val))\n",
    "            \n",
    "            u_i = m(X_0)    \n",
    "\n",
    "            #take mc samples\n",
    "            t_start = time()\n",
    "            mc_i = mc_sampler(x_val,mc_test_batch)\n",
    "            t_end = time()\n",
    "            t_mc += t_end - t_start\n",
    "            u_ref = abs.(max.(mc_i,1e-8))\n",
    "            \n",
    "            #calculate and output errors\n",
    "            errs = vec(abs.(u_i - mc_i))\n",
    "            l1_errs += mean(errs)\n",
    "            l2_errs += mean(errs.^2)\n",
    "            li_errs = max(li_errs, maximum(errs))\n",
    "            rel_errs = errs ./ u_ref\n",
    "            rel_l1_errs += mean(rel_errs)\n",
    "            rel_l2_errs += mean(rel_errs.^2)    \n",
    "            rel_li_errs = max(rel_li_errs,maximum(rel_errs))\n",
    "            \n",
    "        end\n",
    "\n",
    "        #find means\n",
    "        t_mc = t_mc / mc_exp_rounds\n",
    "        l1_errs,l2_errs = l1_errs/mc_exp_rounds, sqrt(l2_errs/mc_exp_rounds)\n",
    "        rel_l1_errs,rel_l2_errs = rel_l1_errs/mc_exp_rounds, sqrt(rel_l2_errs/mc_exp_rounds)  \n",
    "        \n",
    "        #write to file\n",
    "        df_row = DataFrame(step = k,\n",
    "                            l1_errs=l1_errs,l2_errs=l2_errs,li_errs=li_errs,\n",
    "                            rel_l1_errs=rel_l1_errs,rel_l2_errs=rel_l2_errs,rel_li_errs=rel_li_errs,\n",
    "                            t_nn=t_nn,t_mc =t_mc)\n",
    "        \n",
    "        CSV.write(file_name, df_row, append = true)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function generate_training_data(X_init,x_sde,x_phi)    \n",
    "        X_0 = Array(normalise(X_init))\n",
    "        X_sde = x_sde(X_init,batch_samples)\n",
    "        y_train = x_phi(X_sde)\n",
    "        return [(X_0,y_train)]\n",
    "    end\n",
    "\n",
    "    #define network layers\n",
    "    input = Dense(d, d + 20, identity; \n",
    "                           bias = false, \n",
    "                           init = glorot_normal)\n",
    "\n",
    "    hidden = Dense(d + 20, d + 20, identity;\n",
    "                            bias = false,\n",
    "                            init = glorot_normal)\n",
    "\n",
    "    #no activation on the last layer\n",
    "    output = Dense(d + 20,1,identity)\n",
    "\n",
    "    batch_norm_layer = BatchNorm(d + 20, tanh;\n",
    "                                            initβ = zeros, \n",
    "                                            initγ = ones,\n",
    "                                            ϵ = 1e-6, \n",
    "                                            momentum = 0.01)\n",
    "    \n",
    "    #define network architecture\n",
    "    m = Chain(input,\n",
    "           #     batch_norm_layer,\n",
    "                hidden,\n",
    "          #      batch_norm_layer,\n",
    "        #        hidden,\n",
    "        #        batch_norm_layer,\n",
    "                output)\n",
    "    \n",
    "    #loss function = \n",
    "    loss(u,v) = mean((m(u) - v).^2)\n",
    "    \n",
    "    ps = Flux.params(m)\n",
    "\n",
    "    opt = Optimiser(ExpDecay(learning_rate,0.1,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "    #set to train mode\n",
    "    trainmode!(m)\n",
    "    \n",
    "    #generate initial training data\n",
    "    X_init = X_sampler(batch_samples)\n",
    "    data = generate_training_data(X_init,x_sde,x_phi)\n",
    "    \n",
    "    k_iter_output(0, 0) #compare with MC at this stage\n",
    "\n",
    "    #start training time counter\n",
    "    t_nn_start = time()\n",
    "    \n",
    "    for k in 1:train_steps\n",
    "        \n",
    "        #generate new training data\n",
    "        X_init = X_sampler(batch_samples)\n",
    "        data = generate_training_data(X_init,x_sde,x_phi)\n",
    "\n",
    "        #learning step\n",
    "        train!(loss,ps,data,opt)\n",
    "\n",
    "        #output the errors and timings at these steps\n",
    "        if mod(k,err_step) == 0 \n",
    "            \n",
    "            t_nn_end = time()\n",
    "            t_nn = t_nn_end - t_nn_start #timer for the training steps\n",
    "            testmode!(m)\n",
    "            k_iter_output(t_nn, k) #compare with MC at this stage\n",
    "            t_nn_start = time() #start new training timer\n",
    "            trainmode!(m) #set back train mode\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    print(\"Output ready\")\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "nn_estimation(\"stochastic_lorenz_results_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
