{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "time 1.0\n",
      "v_i output1.5697021981161812\n",
      "#################\n",
      "Iteration 1\n",
      "time 0.875\n",
      "v_i output1.5684641029119015\n",
      "#################\n",
      "Iteration 2\n",
      "time 0.75\n",
      "v_i output1.5625712409239978\n",
      "#################\n",
      "Iteration 3\n",
      "time 0.625\n",
      "v_i output1.5608615538468062\n",
      "#################\n",
      "Iteration 4\n",
      "time 0.5\n",
      "v_i output1.5596993616690438\n",
      "#################\n",
      "Iteration 5\n",
      "time 0.375\n",
      "v_i output1.557649229891107\n",
      "#################\n",
      "Iteration 6\n",
      "time 0.25\n",
      "v_i output1.5560061608311997\n",
      "#################\n",
      "Iteration 7\n",
      "time 0.125\n",
      "v_i output1.5553784370422363\n",
      "#################\n"
     ]
    }
   ],
   "source": [
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using ForwardDiff;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using Tracker;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "#initialise error file and create row headers\n",
    "df_row = DataFrame(step = \"step\",value=\"value\")\n",
    "\n",
    "CSV.write(\"hjb_test_v2.csv\", df_row, append = true);\n",
    "\n",
    "\n",
    "N = 8\n",
    "T = 1/3\n",
    "h = T/N\n",
    "\n",
    "#parameters of the function\n",
    "mu_bar = 0\n",
    "sigma_bar = sqrt(2)\n",
    "\n",
    "#dimensions\n",
    "d = 10\n",
    "\n",
    "#data sizes\n",
    "batch_samples = 10000\n",
    "\n",
    "#training epochs parameters\n",
    "train_steps = 6000\n",
    "learning_rate = 0.1 #initial learning rate\n",
    "learn_rate_decrease = 400 #frequency of LR decay\n",
    "\n",
    "#initial condition phi \n",
    "function phi(u)\n",
    "    return sum(u.^2; dims=1).^(0.25)\n",
    "end\n",
    "\n",
    "\n",
    "#non-linear function \n",
    "f(x,y,z) = - sum(z.^2; dims=1)\n",
    "\n",
    "function y_sde(m)  \n",
    "\n",
    "    #y_evolve = Array(zeros((d,batch_samples)))\n",
    "    #y_1 = Array(zeros((d,batch_samples))) \n",
    "    #y_2 = Array(zeros((d,batch_samples))) \n",
    "    \n",
    "    if m == 1\n",
    "        #y_0\n",
    "        y_1 = Array(zeros((d,batch_samples))) \n",
    "    else\n",
    "        y_1 = sigma_bar .* rand(Normal(0,sqrt((m-1)*T/N)),(d, batch_samples))\n",
    "    end\n",
    "    \n",
    "    y_2 = y_1 .+ (sigma_bar .* rand(Normal(0,sqrt(T/N)),(d, batch_samples)))\n",
    "\n",
    "    return y_1,y_2\n",
    "end\n",
    "\n",
    "#define network layers\n",
    "input = Dense(d, d + 10, relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden2 = Dense(d + 10, d + 10 , relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output = Dense(d + 10 ,1,identity)\n",
    "\n",
    "batch_norm_layer = BatchNorm(d + d, identity;\n",
    "                                        initβ = zeros, \n",
    "                                        initγ = ones,\n",
    "                                        ϵ = 1e-6, \n",
    "                                        momentum = 0.9)\n",
    "\n",
    "#define network architecture for fixed model\n",
    "m_fix = Chain(input,\n",
    "    #        batch_norm_layer,\n",
    "            hidden1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden2,\n",
    "    #        batch_norm_layer,\n",
    "            output)\n",
    "\n",
    "#define network layers for a clean set of parameters\n",
    "input_params = Dense(d, d + 10 , relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_params1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_params2 = Dense(d + 10 , d + 10 , relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_params = Dense(d + 10,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_parameters = Chain(input_params,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params2,\n",
    "    #        batch_norm_layer,\n",
    "            output_params)\n",
    "\n",
    "#fixed set of starting parameters for re-setting the nn at each step. \n",
    "ps_start = Flux.params(m_parameters)\n",
    "\n",
    "#define network layers\n",
    "input_train = Dense(d, d + 10 , relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_train1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_train2 = Dense(d + 10, d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_train = Dense(d + 10,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_train = Chain(input_train,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train2,\n",
    "    #        batch_norm_layer,\n",
    "            output_train)\n",
    "\n",
    "\n",
    "loss(u,v) = mean((m_train(u) - v).^2)\n",
    "\n",
    "#initialise V_0\n",
    "V_i = phi\n",
    "\n",
    "y_test = Array(zeros((d,batch_samples)))\n",
    "y_test_mc = Array(zeros((d,batch_samples)))\n",
    "\n",
    "#set the starting point\n",
    "\n",
    "for i in 0:N-1\n",
    "    \n",
    "    println(\"Iteration \", i)\n",
    "    \n",
    "    #reset parameters on trainable model\n",
    "    Flux.loadparams!(m_train, ps_start)     \n",
    "\n",
    "    ps = params(m_train)\n",
    "    \n",
    "    opt = Optimiser(ExpDecay(learning_rate,0.1,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "    #find parameters for nth nn \n",
    "    for k in 1:train_steps\n",
    "\n",
    "        #find the 2 last time steps of SDE\n",
    "        y = y_sde(N - i)\n",
    "        \n",
    "        #format into training data y_1 and y_2\n",
    "        y_n = y[1]\n",
    "        y_n_minus_1 = y[2]\n",
    "                        \n",
    "        #Finding Gradients -\n",
    "        grad_V_i = Array(ones((d,batch_samples)))\n",
    "\n",
    "        for j in 1:batch_samples\n",
    "            grad_V_i_j = ForwardDiff.jacobian(V_i,y_n_minus_1[:,j])\n",
    "            grad_V_i[:,j] = grad_V_i_j\n",
    "        end \n",
    "        \n",
    "         Vv_y = V_i(y_n_minus_1) + h*f(y_n_minus_1,V_i(y_n_minus_1),grad_V_i)\n",
    "\n",
    "        data = [(y_n,Vv_y)]  \n",
    "\n",
    "        #parameter update step\n",
    "        train!(loss,ps,data,opt)\n",
    "        \n",
    "        #manual update to opt at 500 steps\n",
    "        if k== 500 \n",
    "            opt = ADAM(learning_rate/100) #optimiser\n",
    "        end\n",
    "\n",
    "    y_test = y_n\n",
    "    \n",
    "        \n",
    "    end\n",
    "    \n",
    "    println(\"time \", (N - i) / N) \n",
    "    \n",
    "    #load learned params into fixed model\n",
    "    Flux.loadparams!(m_fix, ps)     \n",
    "    \n",
    "    #set new V_i to be the fixed model with new parameters\n",
    "    V_i = m_fix #nn function with parameters \n",
    "    \n",
    "    #take the test value from nn\n",
    "    val = mean(V_i(y_test))\n",
    "    \n",
    "    println(\"v_i output\",val)    \n",
    "    \n",
    "  #  y_test_mc = if i==0 y_test \n",
    "  #  else (y_test .+ sigma_bar .* rand(Normal(0,sqrt(i*T/N)),(d, batch_samples))) end \n",
    "    \n",
    "  #  mc_sample = sum(exp.( - phi(y_test_mc)))./batch_samples\n",
    "    \n",
    "  #  mc_val = -log.(mc_sample)\n",
    "    \n",
    "#\n",
    "#    Include error bars here if u want....\n",
    "    \n",
    "#    #sd_errs = np.sqrt((mean_sq_samples - mean_samples_sq)/mc_samples)\n",
    "#\n",
    "#    #print(\"u_ref = \", -np.log(mean_samples))\n",
    "#\n",
    "#    #print(\"upper end = \", -np.log(mean_samples-sd_errs))\n",
    "#    #print(\"lower end = \", -np.log(mean_samples+sd_errs))\n",
    "#    \n",
    "    \n",
    "  #  println(\"m_c output\",mc_val)    \n",
    "    \n",
    "    println(\"#################\")\n",
    "#    \n",
    "    df_row = DataFrame(step =i,\n",
    "                        value=val)\n",
    "\n",
    "    CSV.write(\"hjb_test_v2.csv\", df_row, append = true)\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
