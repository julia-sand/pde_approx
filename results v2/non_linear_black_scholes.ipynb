{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using ForwardDiff;\n",
    "using Tracker;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "#initialise error file and create row headers\n",
    "df_row = DataFrame(step = \"step\",value=\"value\")\n",
    "\n",
    "#CSV.write(\"deep_pde_approx/non_linear_bs_test_4.csv\", df_row, append = true);\n",
    "\n",
    "N = 96\n",
    "T = 1/3\n",
    "h = T/N\n",
    "\n",
    "#benchmark values for d=10\n",
    "#this is what we want to hit at T=1/3\n",
    "u_ref = 40.7611353\n",
    "\n",
    "#parameters of the function\n",
    "delta, R = 2/3, 0.02 \n",
    "mu_bar , sigma_bar = 0.02,0.2 \n",
    "v_h,v_l = 50,70\n",
    "gamma_h,gamma_l = 0.2,0.02\n",
    "\n",
    "#testing values \n",
    "d = 10\n",
    "#batch_samples = 100\n",
    "train_steps =3000\n",
    "batch_samples = 4096\n",
    "#train_steps = 3000\n",
    "\n",
    "learning_rate = 0.1 #initial learning rate\n",
    "learn_rate_decrease = 2500 #frequency of LR decay\n",
    "\n",
    "y_start = Array(ones((d,batch_samples))) .* 50\n",
    "\n",
    "#initial condition phi \n",
    "function phi(y)\n",
    "    return minimum(y; dims=1)\n",
    "end\n",
    "\n",
    "#non-linear function \n",
    "function f(x,y,z)\n",
    "    return ((-1) * (1 - delta) * (min.(max.(((y .- v_h).*(gamma_h - gamma_l) ./(v_h - v_l) .+ gamma_h)\n",
    "            ,gamma_l),gamma_h)).*y) - (R*y)\n",
    "end\n",
    "\n",
    "#evolution of the SDE using EM\n",
    "function y_sde(m)  \n",
    "\n",
    "    y_sde = Array(ones((d,batch_samples))) * 50\n",
    "    y_1 = Array(ones((d,batch_samples))) * 50\n",
    "    y_2 = Array(ones((d,batch_samples))) * 50\n",
    "    \n",
    "    for i in 1:m \n",
    "        y_sde = y_sde.*((1 + (T/N)*mu_bar) \n",
    "                    .+ (sigma_bar .* rand(Normal(0,sqrt(T/N)),(d, batch_samples))))\n",
    "                        #evolve the SDE for N-n steps\n",
    "        \n",
    "        #one step earlier\n",
    "        if i == m-1 y_1 = y_sde end \n",
    "        #all the way\n",
    "        if i == m y_2 = y_sde end\n",
    "    end        \n",
    "\n",
    "    return y_1,y_2\n",
    "end\n",
    "\n",
    "#define network layers\n",
    "input = Dense(d, d + 10 + 40, relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden1 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden2 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output = Dense(d + 10 + 40,1,identity)\n",
    "\n",
    "batch_norm_layer = BatchNorm(d + d, identity;\n",
    "                                        initβ = zeros, \n",
    "                                        initγ = ones,\n",
    "                                        ϵ = 1e-6, \n",
    "                                        momentum = 0.9)\n",
    "\n",
    "#define network architecture for fixed model\n",
    "m_fix = Chain(input,\n",
    "    #        batch_norm_layer,\n",
    "            hidden1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden2,\n",
    "    #        batch_norm_layer,\n",
    "            output)\n",
    "\n",
    "#define network layers for a clean set of parameters\n",
    "input_params = Dense(d, d + 10 + 40, relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_params1 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_params2 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_params = Dense(d + 10 + 40,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_parameters = Chain(input_params,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params2,\n",
    "    #        batch_norm_layer,\n",
    "            output_params)\n",
    "\n",
    "#fixed set of starting parameters for re-setting the nn at each step. \n",
    "ps_start = Flux.params(m_parameters)\n",
    "#must be a better way to do this???\n",
    "\n",
    "#define network layers\n",
    "input_train = Dense(d, d + 10 + 40, relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_train1 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_train2 = Dense(d + 10 + 40, d + 10 + 40, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_train = Dense(d + 10 + 40,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_train = Chain(input_train,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train2,\n",
    "    #        batch_norm_layer,\n",
    "            output_train)\n",
    "\n",
    "opt = Optimiser(ExpDecay(learning_rate,0.1,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "loss(u,v) = mean((m_train(u) - v).^2)\n",
    "\n",
    "#initialise V_0\n",
    "V_i = phi\n",
    "y_test = Array(ones((d,batch_samples))).*50\n",
    "\n",
    "for i in 0:N\n",
    "        \n",
    "    #reset parameters on trainable model\n",
    "    Flux.loadparams!(m_train, ps_start)     \n",
    "\n",
    "    ps = params(m_train)\n",
    "    \n",
    "    y_input = y_test\n",
    "    \n",
    "    #find parameters for nth nn\n",
    "    for k in 1:train_steps\n",
    "\n",
    "        #find the 2 last time steps of SDE\n",
    "        y = y_sde(N - i + 1)\n",
    "        \n",
    "        #format into training data y_1 and y_2\n",
    "        y_n = y[1]\n",
    "        y_n_minus_1 = y[2]\n",
    "        #if i == N rand(Uniform(-0.5,0.5),(d,batch_samples)) else y[2] end\n",
    "                \n",
    "        ## GRADIENTS METHOD 2\n",
    "                #Finding Gradients -\n",
    "        grad_V_i = Array(ones((d,batch_samples)))\n",
    "\n",
    "        for j in 1:batch_samples\n",
    "            grad_V_i_j = ForwardDiff.jacobian(V_i,y_n_minus_1[:,j])\n",
    "            grad_V_i[:,j] = grad_V_i_j\n",
    "        end \n",
    "        \n",
    "        #calculate the function V\n",
    "        Vv_y = V_i(y_n_minus_1) + h*f(y_n_minus_1,V_i(y_n_minus_1),grad_V_i)\n",
    "\n",
    "        data = [(y_n,Vv_y)]  \n",
    "        \n",
    "        #parameter update step\n",
    "        train!(loss,ps,data,opt)\n",
    "        \n",
    "        #used for calculating the output\n",
    "        y_input = y_n\n",
    "        \n",
    "    end\n",
    "    \n",
    "    #load learned params into fixed model\n",
    "    Flux.loadparams!(m_fix, ps)     \n",
    "    \n",
    "    #set new V_i to be the fixed model with new parameters\n",
    "    V_i = m_fix #nn function with parameters \n",
    "    \n",
    "    # println(\"y_n\" , y_input)\n",
    "    \n",
    "    #take the test value\n",
    "    val = mean(V_i(y_input))\n",
    "    println(\"v_i output\",val, \"step\", i)    \n",
    "    \n",
    "    df_row = DataFrame(step =i,value=val)\n",
    "\n",
    "  #  CSV.write(\"deep_pde_approx/non_linear_bs_test_4.csv\", df_row, append = true)\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
