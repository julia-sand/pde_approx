{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_i output1.5201885478815134\n",
      "v_i output1.4525123668730013\n",
      "v_i output1.2670165923403\n",
      "v_i output1.1685132222585828\n",
      "v_i output1.0351758698890043\n",
      "v_i output0.8859008126094915\n",
      "v_i output0.7266224163872155\n",
      "v_i output0.3542717397212982\n"
     ]
    }
   ],
   "source": [
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using ForwardDiff;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using Tracker;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "#initialise error file and create row headers\n",
    "df_row = DataFrame(step = \"step\",value=\"value\")\n",
    "    #,mc_val = \"mc value\", sd_err = \"sd error\")\n",
    "\n",
    "CSV.write(\"hjb_test.csv\", df_row, append = true);\n",
    "\n",
    "\n",
    "N = 8\n",
    "T = 8/24\n",
    "h = T/N\n",
    "\n",
    "#parameters of the function\n",
    "mu_bar = 0\n",
    "sigma_bar = sqrt(2)\n",
    "\n",
    "#dimensions\n",
    "d = 10\n",
    "\n",
    "#data sizes\n",
    "batch_samples = 256\n",
    "\n",
    "#training epochs parameters\n",
    "train_steps = 600\n",
    "learning_rate = 0.1 #initial learning rate\n",
    "learn_rate_decrease = 400 #frequency of LR decay\n",
    "\n",
    "#initial condition phi \n",
    "function phi(u)\n",
    "    return sum(u.^2; dims=1).^(0.25)\n",
    "end\n",
    "\n",
    "\n",
    "#non-linear function \n",
    "f(x,y,z) = - sum(z.^2; dims=1)\n",
    "\n",
    "function y_sde(m)  \n",
    "\n",
    "    y_evolve = Array(zeros((d,batch_samples)))\n",
    "    y_1 = Array(zeros((d,batch_samples))) \n",
    "    y_2 = Array(zeros((d,batch_samples))) \n",
    "    \n",
    "    for i in 1:m \n",
    "        \n",
    "        y_evolve .+= rand(Normal(0,sqrt(2*T/N)),(d, batch_samples))\n",
    "                        #evolve the SDE for N-n steps\n",
    "        \n",
    "        #one step earlier\n",
    "        if i == m-1 y_1 = y_evolve end \n",
    "        #all the way\n",
    "        if i == m y_2 = y_evolve end\n",
    "    end   \n",
    "    \n",
    "    #see if this makes a difference? \n",
    "#    if m == 1\n",
    "#        y_1 = Array(zeros((d,batch_samples))) \n",
    "#    end\n",
    "\n",
    "    return y_1,y_2\n",
    "end\n",
    "\n",
    "#define network layers\n",
    "input = Dense(d, d + 10, relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden2 = Dense(d + 10, d + 10 , relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output = Dense(d + 10 ,1,identity)\n",
    "\n",
    "batch_norm_layer = BatchNorm(d + d, identity;\n",
    "                                        initβ = zeros, \n",
    "                                        initγ = ones,\n",
    "                                        ϵ = 1e-6, \n",
    "                                        momentum = 0.9)\n",
    "\n",
    "#define network architecture for fixed model\n",
    "m_fix = Chain(input,\n",
    "    #        batch_norm_layer,\n",
    "            hidden1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden2,\n",
    "    #        batch_norm_layer,\n",
    "            output)\n",
    "\n",
    "#define network layers for a clean set of parameters\n",
    "input_params = Dense(d, d + 10 , relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_params1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_params2 = Dense(d + 10 , d + 10 , relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_params = Dense(d + 10,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_parameters = Chain(input_params,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_params2,\n",
    "    #        batch_norm_layer,\n",
    "            output_params)\n",
    "\n",
    "#fixed set of starting parameters for re-setting the nn at each step. \n",
    "ps_start = Flux.params(m_parameters)\n",
    "\n",
    "#define network layers\n",
    "input_train = Dense(d, d + 10 , relu; \n",
    "                       bias = false, \n",
    "                       init = glorot_normal)\n",
    "\n",
    "hidden_train1 = Dense(d + 10 , d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "hidden_train2 = Dense(d + 10, d + 10, relu;\n",
    "                        bias = false,\n",
    "                        init = glorot_normal)\n",
    "\n",
    "#no activation on the last layer\n",
    "output_train = Dense(d + 10,1,identity)\n",
    "\n",
    "#define network architecture for trianing model\n",
    "m_train = Chain(input_train,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train1,\n",
    "    #        batch_norm_layer,\n",
    "            hidden_train2,\n",
    "    #        batch_norm_layer,\n",
    "            output_train)\n",
    "\n",
    "\n",
    "loss(u,v) = mean((m_train(u) - v).^2)\n",
    "\n",
    "#initialise V_0\n",
    "V_i = phi\n",
    "\n",
    "y_test = Array(ones((d,batch_samples)))\n",
    "\n",
    "#set the starting point\n",
    "\n",
    "for i in 1:N\n",
    "    \n",
    "    #reset parameters on trainable model\n",
    "    Flux.loadparams!(m_train, ps_start)     \n",
    "\n",
    "    ps = params(m_train)\n",
    "    \n",
    "    opt = Optimiser(ExpDecay(learning_rate,0.1,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "    #find parameters for nth nn \n",
    "    for k in 1:train_steps\n",
    "\n",
    "        #find the 2 last time steps of SDE\n",
    "        y = y_sde(N - i + 1)\n",
    "        \n",
    "        #format into training data y_1 and y_2\n",
    "        y_n = y[1]\n",
    "        y_n_minus_1 = y[2]\n",
    "                        \n",
    "        #Finding Gradients -\n",
    "        grad_V_i = Array(ones((d,batch_samples)))\n",
    "\n",
    "        for j in 1:batch_samples\n",
    "            grad_V_i_j = ForwardDiff.jacobian(V_i,y_n_minus_1[:,j])\n",
    "            grad_V_i[:,j] = grad_V_i_j\n",
    "        end \n",
    "     #   grad_V_i = ForwardDiff.jacobian(V_i,y_n_minus_1)\n",
    "        \n",
    "       # Vv_y = V_i(y_n_minus_1) + h*f(y_n_minus_1,V_i(y_n_minus_1),grad_V_i)\n",
    "         Vv_y = V_i(y_n_minus_1) + h*f(y_n_minus_1,V_i(y_n_minus_1),grad_V_i)\n",
    "\n",
    "        data = [(y_n,Vv_y)]  \n",
    "\n",
    "        #parameter update step\n",
    "        train!(loss,ps,data,opt)\n",
    "        \n",
    "        #manual update to opt at 500 steps\n",
    "   #     if k== 500 \n",
    "   #         opt = ADAM(learning_rate/100) #optimiser\n",
    "   #     end\n",
    "\n",
    "    y_test = y_n\n",
    "        \n",
    "    end\n",
    "\n",
    "    \n",
    "    #load learned params into fixed model\n",
    "    Flux.loadparams!(m_fix, ps)     \n",
    "    \n",
    "    #set new V_i to be the fixed model with new parameters\n",
    "    V_i = m_fix #nn function with parameters \n",
    "    \n",
    "    #take the test value from nn\n",
    "    val = mean(V_i(y_test))\n",
    "    \n",
    "    println(\"v_i output\",val)    \n",
    " #   println(\"y_n\", mean(y_n;dims = 2))\n",
    "    \n",
    "    df_row = DataFrame(step =i,\n",
    "                        value=val)\n",
    "\n",
    "    CSV.write(\"hjb_test.csv\", df_row, append = true)\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
