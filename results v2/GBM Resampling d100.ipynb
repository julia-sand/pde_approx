{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ready!Output ready!"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "function nn_estimation(file_name,file_name1,file_name2,batch_samples)    \n",
    "    \n",
    "    #parameters of the function\n",
    "    r = Float16(0.05);\n",
    "    T = Int(1);  #ttm\n",
    "    N = Int(1);  #number of evolution steps\n",
    "    K = Int(100); #strike\n",
    "    mu = Float16(-0.05); \n",
    "\n",
    "    d = Int(100); #number of dimensions\n",
    "    #batch_samples = Int(800); #total samples in batch\n",
    "    a = 90;\n",
    "    b = 110;\n",
    "    initial_sampler = Uniform(a,b);\n",
    "    \n",
    "    time_grid = LinRange(1/d, 1, d); \n",
    "    sigma = 0.1 .+ 0.5 .* time_grid; #define changing variance on the time grid\n",
    "\n",
    "    mc_samples = Int(500000); #number of samples to take for Monte-Carlo approximation\n",
    "    mc_exp_rounds = Int(1); #number of times to repeat MC for the error average\n",
    "    \n",
    "    learning_rate = Float16(0.001); #initial learning rate\n",
    "    learn_rate_decrease = 250000; #how frequently to decay learning rate\n",
    "\n",
    "    train_steps = Int(200000); #total number of training epochs\n",
    "    err_step = Int(25000); #after how many training steps to compare errors\n",
    "\n",
    "    #placeholder\n",
    "    t_mc = 0\n",
    "    \n",
    "    function x_sde(X::Array) #discretisation of SDE \n",
    "        #how_many_samples = length(X)\n",
    "        for _ in 1:N\n",
    "            #take appropriate samples from normal \n",
    "            eps = rand(Normal(0,1),(d, batch_samples))\n",
    "            X = X .* (exp.((mu .- 0.5*sigma.^2)*(T/N) .+  sqrt(T/N).*( sigma .* eps)))\n",
    "       # println(eps)\n",
    "        end\n",
    "        return X\n",
    "    end; \n",
    "\n",
    "    function x_phi(x::Array) #function to use with FK expectation /initial conditions\n",
    "        phi_(x) = exp(-r*T) * max((maximum(x)-K),0)\n",
    "        mapslices(phi_, x; dims =1)\n",
    "    end;\n",
    "    \n",
    "    #standard normalisation function \n",
    "    function norm_ab(y)\n",
    "        mid_point = (a + b) / 2\n",
    "        y_norm(y) = (y .- mid_point) ./ (b-a)\n",
    "        return mapslices(y_norm,y;dims =1)\n",
    "    end\n",
    "    \n",
    "    #initialise error file and create row headers\n",
    "    #three separate holders for errors\n",
    "    df_row_each = DataFrame(step = \"step\",\n",
    "                        l1_errs=\"l1_errs\",l2_errs=\"l2_errs\",li_errs=\"li_errs\",\n",
    "                        rel_l1_errs=\"rel_l1_errs\",rel_l2_errs=\"rel_l2_errs\",rel_li_errs=\"rel_li_errs\")\n",
    "\n",
    "    df_row = DataFrame(step = \"step\",\n",
    "                        l1_errs=\"l1_errs\",\n",
    "                        rel_l1_errs=\"rel_l1_errs\")\n",
    "    \n",
    "    CSV.write(file_name, df_row_each, append = true);\n",
    "\n",
    "    CSV.write(file_name1, df_row, append = true);\n",
    "\n",
    "    CSV.write(file_name2, df_row, append = true);\n",
    "\n",
    "    \n",
    "    #calculate errors and write to file\n",
    "    function k_iter_output(k)\n",
    "        \n",
    "        #generate test batch\n",
    "        X_test = rand(initial_sampler,(d,batch_samples))\n",
    "        X_test_norm = norm_ab(X_test)\n",
    "        \n",
    "        #generate mc data\n",
    "        function mc_sampler(X_test)\n",
    "            x_mc_store = zeros((1,batch_samples))\n",
    "            for _ in 1:mc_samples\n",
    "                x_mc_store += Array((x_phi(x_sde(X_init))))\n",
    "            end\n",
    "            phi_mc = x_mc_store ./ mc_samples;\n",
    "            return Array(phi_mc)\n",
    "        end\n",
    "                   \n",
    "        #initial errors for finding the mean\n",
    "#        l1_errs = 0.\n",
    "#        rel_l1_errs = 0.\n",
    "\n",
    "        l1_errs,l2_errs,li_errs = 0.,0.,0.\n",
    "        rel_l1_errs, rel_l2_errs, rel_li_errs = 0., 0., 0.\n",
    "\n",
    "        l1_errs1 = 0.\n",
    "        rel_l1_errs1 = 0.\n",
    "        \n",
    "        l1_errs2 = 0.\n",
    "        rel_l1_errs2 = 0.\n",
    "        \n",
    "        #run through testmode NN for comparison\n",
    "        testmode!(m)\n",
    "        u_i = m(X_test_norm)\n",
    "        \n",
    "        #run through testmode NN for comparison\n",
    "        testmode!(m1)\n",
    "        u_i1 = m1(X_test_norm)\n",
    "\n",
    "        testmode!(m2)\n",
    "        u_i2 = m2(X_test_norm)\n",
    "\n",
    "        for _ in 1:mc_exp_rounds\n",
    "\n",
    "            #take mc samples\n",
    "            t_start = time()\n",
    "            mc_i = mc_sampler(X_test)\n",
    "            t_end = time()\n",
    "            t_mc += t_end - t_start\n",
    "            u_ref = abs.(max.(mc_i,1e-8))\n",
    "            \n",
    "            #calculate and output errors\n",
    "            errs = vec(abs.(u_i - mc_i))\n",
    "            l1_errs += mean(errs)\n",
    "            rel_errs = errs ./ u_ref\n",
    "            rel_l1_errs += mean(rel_errs)\n",
    "            l2_errs += mean(errs.^2)\n",
    "            li_errs = max(li_errs, maximum(errs))\n",
    "            rel_l2_errs += mean(rel_errs.^2)    \n",
    "            rel_li_errs = max(rel_li_errs,maximum(rel_errs))\n",
    "\n",
    "            errs1 = vec(abs.(u_i1 - mc_i))\n",
    "            l1_errs1 += mean(errs1)\n",
    "            rel_errs1 = errs1 ./ u_ref\n",
    "            rel_l1_errs1 += mean(rel_errs1)\n",
    "\n",
    "            errs2 = vec(abs.(u_i2 - mc_i))\n",
    "            l1_errs2 += mean(errs2)\n",
    "            rel_errs2 = errs2 ./ u_ref\n",
    "            rel_l1_errs2 += mean(rel_errs2)\n",
    "            \n",
    "        end\n",
    "\n",
    "        #find means\n",
    "#        l1_errs = l1_errs/mc_exp_rounds\n",
    "#        rel_l1_errs= rel_l1_errs/mc_exp_rounds  \n",
    "        l1_errs,l2_errs = l1_errs/mc_exp_rounds, sqrt(l2_errs/mc_exp_rounds)\n",
    "        rel_l1_errs,rel_l2_errs = rel_l1_errs/mc_exp_rounds, sqrt(rel_l2_errs/mc_exp_rounds)  \n",
    "                    \n",
    "        l1_errs1 = l1_errs1/mc_exp_rounds\n",
    "        rel_l1_errs1 = rel_l1_errs1/mc_exp_rounds  \n",
    "\n",
    "        l1_errs2 = l1_errs2/mc_exp_rounds\n",
    "        rel_l1_errs2 = rel_l1_errs2/mc_exp_rounds  \n",
    "        \n",
    "        #write to file\n",
    "#        df_row = DataFrame(step = k,\n",
    "#                            l1_errs=l1_errs,\n",
    "#                            rel_l1_errs=rel_l1_errs)\n",
    "#\n",
    "        #write to file\n",
    "        df_row = DataFrame(step = k,\n",
    "                            l1_errs=l1_errs,l2_errs=l2_errs,li_errs=li_errs,\n",
    "                            rel_l1_errs=rel_l1_errs,rel_l2_errs=rel_l2_errs,\n",
    "                            rel_li_errs=rel_li_errs)\n",
    "\n",
    "        df_row1 = DataFrame(step = k,\n",
    "                    l1_errs=l1_errs1,\n",
    "                    rel_l1_errs=rel_l1_errs1)\n",
    "        \n",
    "        df_row2 = DataFrame(step = k,\n",
    "                    l1_errs=l1_errs2,\n",
    "                    rel_l1_errs=rel_l1_errs2)\n",
    "\n",
    "        CSV.write(file_name, df_row, append = true)\n",
    "        CSV.write(file_name1, df_row1, append = true)\n",
    "        CSV.write(file_name2, df_row2, append = true)\n",
    "\n",
    "    end\n",
    "    \n",
    "    function generate_training_data(X_init,x_sde,x_phi)    \n",
    "        X_0 = Array(norm_ab(X_init))\n",
    "        X_sde = x_sde(X_init)\n",
    "        y_train = x_phi(X_sde)\n",
    "        return [(X_0,y_train)]\n",
    "    end\n",
    "\n",
    "    #define network layers\n",
    "    input = Dense(d, d + d, tanh; \n",
    "                           bias = false, \n",
    "                           init = glorot_normal)\n",
    "\n",
    "    hidden = Dense(d + d, d + d, tanh;\n",
    "                            bias = false,\n",
    "                            init = glorot_normal)\n",
    "\n",
    "    #no activation on the last layer\n",
    "    output = Dense(d + d,1,identity)\n",
    "    \n",
    "    #define network layers\n",
    "    input1 = Dense(d, d + d, tanh; \n",
    "                           bias = false, \n",
    "                           init = glorot_normal)\n",
    "\n",
    "    hidden1 = Dense(d + d, d + d, tanh;\n",
    "                            bias = false,\n",
    "                            init = glorot_normal)\n",
    "\n",
    "    #no activation on the last layer\n",
    "    output1 = Dense(d + d,1,identity)\n",
    "    \n",
    "    #define network layers\n",
    "    input2 = Dense(d, d + d, tanh; \n",
    "                           bias = false, \n",
    "                           init = glorot_normal)\n",
    "\n",
    "    hidden2 = Dense(d + d, d + d, tanh;\n",
    "                            bias = false,\n",
    "                            init = glorot_normal)\n",
    "\n",
    "    #no activation on the last layer\n",
    "    output2 = Dense(d + d,1,identity)\n",
    "\n",
    "    \n",
    "    #define network architecture\n",
    "    m = Chain(input,\n",
    "        #        batch_norm_layer,\n",
    "                hidden,\n",
    "        #        batch_norm_layer,\n",
    "        #        hidden,\n",
    "        #        batch_norm_layer,\n",
    "                output)\n",
    "    \n",
    "    #define network architecture\n",
    "    m1 = Chain(input1,\n",
    "        #        batch_norm_layer,\n",
    "                hidden1,\n",
    "        #        batch_norm_layer,\n",
    "        #        hidden,\n",
    "        #        batch_norm_layer,\n",
    "                output1)\n",
    "        \n",
    "    #define network architecture\n",
    "    m2 = Chain(input2,\n",
    "        #        batch_norm_layer,\n",
    "                hidden2,\n",
    "        #        batch_norm_layer,\n",
    "        #        hidden,\n",
    "        #        batch_norm_layer,\n",
    "                output2)\n",
    "    \n",
    "   #loss function = \n",
    "    loss(u,v) = mean((m(u) - v).^2)\n",
    "    loss1(u,v) = mean((m1(u) - v).^2)\n",
    "    loss2(u,v) = mean((m2(u) - v).^2)\n",
    "    \n",
    "    ps = Flux.params(m)\n",
    "    ps1 = Flux.params(m1)\n",
    "    ps2 = Flux.params(m2)\n",
    "\n",
    "    opt = Optimiser(ExpDecay(learning_rate,0.1,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "    #set to train mode\n",
    "    trainmode!(m)\n",
    "    trainmode!(m1)\n",
    "    trainmode!(m2)\n",
    "    \n",
    "    #generate initial training data\n",
    "    X_init = rand(initial_sampler,(d,batch_samples))\n",
    "    data = generate_training_data(X_init,x_sde,x_phi)\n",
    "\n",
    "    X_init_0 = X_init\n",
    "    \n",
    "    data1 = data\n",
    "    data2 = data\n",
    "    \n",
    "    k_iter_output(0) #compare with MC at this stage\n",
    "\n",
    "    #start training time counter\n",
    "    t_nn_start = time()\n",
    "    \n",
    "    for k in 1:train_steps\n",
    "        \n",
    "        #generate new training data each epoch\n",
    "        X_init = rand(initial_sampler,(d,batch_samples))\n",
    "        data = generate_training_data(X_init,x_sde,x_phi)\n",
    "        \n",
    "        #re.evolve SDE at current X\n",
    "        data2 = generate_training_data(X_init_0,x_sde,x_phi)\n",
    "        \n",
    "        #learning step\n",
    "        train!(loss,ps,data,opt)\n",
    "        train!(loss1,ps1,data1,opt)\n",
    "        train!(loss2,ps2,data2,opt)\n",
    "        \n",
    "        #output the errors and timings at these steps\n",
    "        if mod(k,err_step) == 0 \n",
    "            \n",
    "            t_nn_end = time()\n",
    "            t_nn = t_nn_end - t_nn_start #timer for the training steps\n",
    "            k_iter_output(k) #compare with MC at this stage\n",
    "            t_nn_start = time() #start new training timer\n",
    "            trainmode!(m) #set back train mode\n",
    "            trainmode!(m1) #set back train mode\n",
    "            trainmode!(m2) #set back train mode\n",
    "            \n",
    "            #reset x and fix phi every 25k steps\n",
    "            X_init_0 = rand(initial_sampler,(d,batch_samples))\n",
    "            data1 = generate_training_data(X_init_0,x_sde,x_phi)\n",
    "\n",
    "        end\n",
    "        \n",
    "\n",
    "    end\n",
    "    \n",
    "    print(\"Output ready!\")\n",
    "\n",
    "end\n",
    "\n",
    "nn_estimation(\"gbm_eachx_d100_batch400.csv\",\"gbm_onex_d100_batch400.csv\",\"gbm_onex_eachphi_d100_batch400.csv\",400);\n",
    "nn_estimation(\"gbm_eachx_d100_batch100.csv\",\"gbm_onex_d100_batch100.csv\",\"gbm_onex_eachphi_d100_batch100.csv\",100);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_estimation(\"gbm_eachx_d100_batch1000.csv\",\"gbm_onex_d100_batch1000.csv\",\"gbm_onex_eachphi_d100_batch1000.csv\",1000);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
