{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "using Distributions;\n",
    "using Random;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using Statistics;\n",
    "using LinearAlgebra;\n",
    "using Flux;\n",
    "using Flux: params, \n",
    "            Dense, \n",
    "            Chain, \n",
    "            glorot_normal, \n",
    "            normalise, \n",
    "            Optimiser,\n",
    "            train!;\n",
    "\n",
    "\n",
    "function nn_estimation(file_name)\n",
    "        \n",
    "    #parameters of the function\n",
    "    r = Float16(0.05);\n",
    "    T = Int(1);  #ttm\n",
    "    N= Int(1)\n",
    "    K = Int(100); #strike\n",
    "    mu = Float16(-0.05); \n",
    "    a = 90;\n",
    "    b = 110;\n",
    "\n",
    "    #standard normalisation function \n",
    "    function norm_ab(y)\n",
    "        mid_point = (a + b) / 2\n",
    "        y_norm(y) = (y .- mid_point) ./ (b-a)\n",
    "        return mapslices(y_norm,y;dims =1)\n",
    "    end\n",
    "    \n",
    "    d = Int(100); #number of timesteps\n",
    "    batch_samples = Int(800); #total samples in batch\n",
    "    initial_sampler = Uniform(a,b);\n",
    "    \n",
    "    time_grid = LinRange(1/d, 1, d); \n",
    "  #  sigma = 0.1 .+ 0.5 .* time_grid; #define changing variance on the time grid\n",
    "    beta = 0.1 .+ 0.5 .* time_grid; #different variance for each component\n",
    "    Q = ones((d,d)) * 0.5\n",
    "    Q[diagind(Q)] .= 1\n",
    "    sigma = Array(cholesky(Q).L)\n",
    "    sigma_norms = sqrt.(sum(sigma.^2;dims=2))\n",
    "\n",
    "    mc_samples = Int(500000); #number of samples to take for Monte-Carlo approximation\n",
    "    mc_exp_rounds = Int(1); #number of times to repeat MC for the error average\n",
    "    \n",
    "    learning_rate = Float16(0.001); #initial learning rate\n",
    "    learn_rate_decrease = 250000; #how frequently to decay learning rate\n",
    "\n",
    "    train_steps = Int(750000); #total number of training epochs\n",
    "    err_step = Int(25000); #after how many training steps to compare errors\n",
    "\n",
    "    function x_sde(X::Array) #discretisation of SDE \n",
    "        for _ in 1:N\n",
    "            eps = rand(Normal(0,1),(d,1))\n",
    "            X = X .* exp.((mu .- 0.5*(beta.*sigma_norms).^2).* T \n",
    "            .+ (beta.*(sigma*eps)))\n",
    "        end\n",
    "        return X\n",
    "    end;\n",
    "    \n",
    "#    function x_sde(X::Array) #discretisation of SDE \n",
    "#        sde(X) = X .* (exp.((mu .- 0.5*sigma.^2)*T + ( sqrt(T)* sigma .* rand(Normal(0,1),d))))\n",
    "#        mapslices(sde, X; dims =1)\n",
    "#    end;\n",
    "\n",
    "    function x_phi(x::Array) #function to use with FK expectation\n",
    "        phi_(x) = exp(-mu*T) * max((K - minimum(x)),0)\n",
    "        mapslices(phi_, x; dims =1)\n",
    "    end;\n",
    "    \n",
    "    #initialise error file and create row headers\n",
    "    df_row = DataFrame(step = \"step\",\n",
    "                        l1_errs=\"l1_errs\",l2_errs=\"l2_errs\",li_errs=\"li_errs\",\n",
    "                        rel_l1_errs=\"rel_l1_errs\",rel_l2_errs=\"rel_l2_errs\",rel_li_errs=\"rel_li_errs\",\n",
    "                        t_nn=\"t_nn\",t_mc =\"t_mc\")\n",
    "    \n",
    "    CSV.write(file_name, df_row, append = true);\n",
    "\n",
    "    \n",
    "    #calculate errors and write to file\n",
    "    function k_iter_output(X_init, t_nn, k)\n",
    "        \n",
    "        #generate mc data\n",
    "        function mc_sampler(X_init)\n",
    "            x_mc_store = zeros((1,batch_samples))\n",
    "            for _ in 1:mc_samples\n",
    "                x_mc_store += Array((x_phi(x_sde(X_init))))\n",
    "            end\n",
    "            phi_mc = x_mc_store ./ mc_samples;\n",
    "            return Array(phi_mc)\n",
    "        end\n",
    "           \n",
    "        #find the expected error vs mc samples\n",
    "        \n",
    "        #initial errors for finding the mean\n",
    "        t_mc = 0 \n",
    "        l1_errs,l2_errs,li_errs = 0.,0.,0.\n",
    "        rel_l1_errs, rel_l2_errs, rel_li_errs = 0., 0., 0.\n",
    "        \n",
    "        #run through testmode NN for comparison\n",
    "        X_0 = Array(norm_ab(X_init))\n",
    "        testmode!(m)\n",
    "        u_i = m(X_0)    \n",
    "        \n",
    "        for _ in 1:mc_exp_rounds\n",
    "\n",
    "            #take mc samples\n",
    "            t_start = time()\n",
    "            mc_i = mc_sampler(X_init)\n",
    "            t_end = time()\n",
    "            t_mc += t_end - t_start\n",
    "            u_ref = abs.(max.(mc_i,1e-8))\n",
    "            \n",
    "            #calculate and output errors\n",
    "            errs = vec(abs.(u_i - mc_i))\n",
    "            l1_errs += mean(errs)\n",
    "            l2_errs += mean(errs.^2)\n",
    "            li_errs = max(li_errs, maximum(errs))\n",
    "            rel_errs = errs ./ u_ref\n",
    "            rel_l1_errs += mean(rel_errs)\n",
    "            rel_l2_errs += mean(rel_errs.^2)    \n",
    "            rel_li_errs = max(rel_li_errs,maximum(rel_errs))\n",
    "            \n",
    "        end\n",
    "\n",
    "        #find means\n",
    "        t_mc = t_mc / mc_exp_rounds\n",
    "        l1_errs,l2_errs = l1_errs/mc_exp_rounds, sqrt(l2_errs/mc_exp_rounds)\n",
    "        rel_l1_errs,rel_l2_errs = rel_l1_errs/mc_exp_rounds, sqrt(rel_l2_errs/mc_exp_rounds)  \n",
    "        \n",
    "        #write to file\n",
    "        df_row = DataFrame(step = k,\n",
    "                            l1_errs=l1_errs,l2_errs=l2_errs,li_errs=li_errs,\n",
    "                            rel_l1_errs=rel_l1_errs,rel_l2_errs=rel_l2_errs,rel_li_errs=rel_li_errs,\n",
    "                            t_nn=t_nn,t_mc =t_mc)\n",
    "        \n",
    "        CSV.write(file_name, df_row, append = true)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function generate_training_data(X_init,x_sde,x_phi)    \n",
    "        X_0 = Array(norm_ab(X_init))\n",
    "        X_sde = x_sde(X_init)\n",
    "        y_train = x_phi(X_sde)\n",
    "        return [(X_0,y_train)]\n",
    "    end\n",
    "\n",
    "    #define network layers\n",
    "    input = Dense(d, d + d, tanh; \n",
    "                           bias = false, \n",
    "                           init = glorot_normal)\n",
    "\n",
    "    hidden = Dense(d + d, d + d, tanh;\n",
    "                            bias = false,\n",
    "                            init = glorot_normal)\n",
    "\n",
    "    #no activation on the last layer\n",
    "    output = Dense(d + d,1,identity)\n",
    "\n",
    "    batch_norm_layer = BatchNorm(d + d, identity;\n",
    "                                            initβ = zeros, \n",
    "                                            initγ = ones,\n",
    "                                            ϵ = 1e-6, \n",
    "                                            momentum = 0.9)\n",
    "    \n",
    "    #define network architecture\n",
    "    m = Chain(input,\n",
    "         #       batch_norm_layer,\n",
    "                hidden,\n",
    "         #       batch_norm_layer,\n",
    "        #        hidden,\n",
    "        #        batch_norm_layer,\n",
    "                output)\n",
    "    \n",
    "    #loss function = \n",
    "    loss(u,v) = mean((m(u) - v).^2)\n",
    "    \n",
    "    ps = Flux.params(m)\n",
    "\n",
    "    opt = Optimiser(ExpDecay(learning_rate,0.01,learn_rate_decrease,1e-8),ADAM()) #optimiser\n",
    "\n",
    "    #set to train mode\n",
    "    trainmode!(m)\n",
    "    \n",
    "    #generate initial training data\n",
    "    X_init = rand(initial_sampler,(d,batch_samples))\n",
    "    data = generate_training_data(X_init,x_sde,x_phi)\n",
    "    \n",
    "    k_iter_output(X_init, 0, 0) #compare with MC at this stage\n",
    "\n",
    "    #start training time counter\n",
    "    t_nn_start = time()\n",
    "    \n",
    "    for k in 1:train_steps\n",
    "        \n",
    "        #generate new training data\n",
    "        X_init = rand(initial_sampler,(d,batch_samples))\n",
    "        data = generate_training_data(X_init,x_sde,x_phi)\n",
    "\n",
    "        #learning step\n",
    "        train!(loss,ps,data,opt)\n",
    "\n",
    "        #output the errors and timings at these steps\n",
    "        if mod(k,err_step) == 0 \n",
    "            \n",
    "            t_nn_end = time()\n",
    "            t_nn = t_nn_end - t_nn_start #timer for the training steps\n",
    "            k_iter_output(X_init, t_nn, k) #compare with MC at this stage\n",
    "            t_nn_start = time() #start new training timer\n",
    "            trainmode!(m) #set back train mode\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    print(\"Output ready\")\n",
    "\n",
    "end\n",
    "\n",
    "nn_estimation(\"corr_gbm_d100_validation_errs.csv\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
